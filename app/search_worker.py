# /app/kafka_worker.py
import json
import logging
from kafka import KafkaConsumer, KafkaProducer
from kafka.errors import KafkaError
from app.services.search_service import SearchService
from app.core.config import settings
import uuid
from datetime import datetime, timezone
import os

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    """
    H√†m ch√≠nh ƒë·ªÉ kh·ªüi t·∫°o v√† ch·∫°y Kafka worker.
    - L·∫Øng nghe c√°c y√™u c·∫ßu t√¨m ki·∫øm t·ª´ topic 'search_requests'.
    - G·ªçi SearchService ƒë·ªÉ x·ª≠ l√Ω.
    - G·ª≠i k·∫øt qu·∫£ v√† log ƒë·∫øn c√°c topic Kafka t∆∞∆°ng ·ª©ng.
    """
    logger.info("=============================================")
    logger.info("      üöÄ Search Worker Starting üöÄ")
    logger.info("=============================================")

    # Kh·ªüi t·∫°o KafkaConsumer ƒë·ªÉ l·∫Øng nghe y√™u c·∫ßu
    try:
        consumer = KafkaConsumer(
            settings.SEARCH_REQUESTS_TOPIC,
            bootstrap_servers=settings.KAFKA_BROKER_URL,
            auto_offset_reset='earliest',
            enable_auto_commit=True,
            group_id='search-worker-group',
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )
        logger.info("‚úÖ KafkaConsumer connected successfully.")
    except KafkaError as e:
        logger.error(f"‚ùå CRITICAL: Could not connect KafkaConsumer: {e}")
        return # Tho√°t n·∫øu kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c

    # Kh·ªüi t·∫°o KafkaProducer ƒë·ªÉ g·ª≠i ph·∫£n h·ªìi
    try:
        producer = KafkaProducer(
            bootstrap_servers=settings.KAFKA_BROKER_URL,
            value_serializer=lambda v: json.dumps(v, default=str).encode('utf-8') # H·ªó tr·ª£ serialize UUID
        )
        logger.info("‚úÖ KafkaProducer connected successfully.")
    except KafkaError as e:
        logger.error(f"‚ùå CRITICAL: Could not connect KafkaProducer: {e}")
        return

    # Kh·ªüi t·∫°o SearchService ƒë·ªÉ t√°i s·ª≠ d·ª•ng logic t√¨m ki·∫øm
    try:
        search_service = SearchService()
        logger.info("‚úÖ SearchService initialized successfully.")
    except Exception as e:
        logger.error(f"‚ùå CRITICAL: Failed to initialize SearchService: {e}")
        return

    logger.info("=============================================")
    logger.info("üëÇ Worker is now listening for messages...")
    logger.info("=============================================")


    # V√≤ng l·∫∑p v√¥ t·∫≠n ƒë·ªÉ x·ª≠ l√Ω message
    for message in consumer:
        try:
            request_data = message.value
            query_text = request_data.get("query_text")
            request_id = request_data.get("request_id") # Nh·∫≠n request_id t·ª´ message
            user_id = request_data.get("user_id") # Nh·∫≠n user_id
            limit = request_data.get("limit", 20)

            if not query_text or not request_id:
                logger.warning(f"‚ö†Ô∏è Received message with missing 'query_text' or 'request_id'. Skipping.")
                continue

            logger.info(f"üì¨ Received search request | RequestID: {request_id} | Query: '{query_text}'")

            # G·ªçi logic t√¨m ki·∫øm t·ª´ SearchService
            search_results = search_service.search_semantic(query=query_text, limit=limit)

            # --- GIAI ƒêO·∫†N 3: G·ª¨I K·∫æT QU·∫¢ V√Ä LOGGING ---

            # 1. G·ª≠i k·∫øt qu·∫£ t√¨m ki·∫øm v√†o topic 'search_results'
            result_payload = {
                "request_id": request_id,
                "products": search_results
            }
            logger.info(f"--- G·ª¨I PAYLOAD L√äN KAFKA (RequestID: {request_id}) ---")
            logger.info(json.dumps(result_payload, default=str, indent=4, ensure_ascii=False))

            producer.send(settings.SEARCH_RESULTS_TOPIC, value=result_payload)
            logger.info(f"üì§ Sent {len(search_results)} results to '{settings.SEARCH_RESULTS_TOPIC}' for RequestID: {request_id}")

            # 2. G·ª≠i d·ªØ li·ªáu log v√†o topic 'search_logging_events'
            ranked_ids = [result['Id'] for result in search_results]
            log_payload = {
                "search_id": str(uuid.uuid4()),
                "user_id": user_id,
                "query_text": query_text,
                "result_count": len(search_results),
                "ranked_product_ids": ranked_ids,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            producer.send(settings.SEARCH_LOGGING_TOPIC, value=log_payload)
            logger.info(f"üìù Sent log event to '{settings.SEARCH_LOGGING_TOPIC}' for RequestID: {request_id}")

            # ƒê·∫£m b·∫£o message ƒë∆∞·ª£c g·ª≠i ƒëi
            producer.flush()

        except json.JSONDecodeError:
            logger.error("Failed to decode message value. Skipping.")
        except Exception as e:
            logger.error(f"An unexpected error occurred while processing message: {e}", exc_info=True)


if __name__ == "__main__":
    main()